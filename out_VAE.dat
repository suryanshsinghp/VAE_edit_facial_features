device is cuda
load from previous checkpoint:  False
epoch start number:  0
Total number of epochs:  20
loss function:  mse
batch size:  128
image size is :  64
saving every  2  num of epochs
track loss based on train or test?:  train
learning rate:  0.0005
stop learning when diff between two successive loss is less than:  1e-06
latent dimension:  128
beta:  0.0008
saving  3  num of train and test image at every epoch
deleting previous data
selected batch size 128
Files already downloaded and verified
Files already downloaded and verified
Number of training examples: 162770
size of images:  torch.Size([3, 64, 64])
shape of input image is : torch.Size([3, 64, 64])
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
betaVAE                                  [128, 3, 64, 64]          --
├─Conv2d: 1-1                            [128, 32, 32, 32]         896
├─BatchNorm2d: 1-2                       [128, 32, 32, 32]         64
├─ReLU: 1-3                              [128, 32, 32, 32]         --
├─Conv2d: 1-4                            [128, 64, 16, 16]         18,496
├─BatchNorm2d: 1-5                       [128, 64, 16, 16]         128
├─ReLU: 1-6                              [128, 64, 16, 16]         --
├─Conv2d: 1-7                            [128, 128, 8, 8]          73,856
├─BatchNorm2d: 1-8                       [128, 128, 8, 8]          256
├─ReLU: 1-9                              [128, 128, 8, 8]          --
├─Conv2d: 1-10                           [128, 256, 4, 4]          295,168
├─BatchNorm2d: 1-11                      [128, 256, 4, 4]          512
├─ReLU: 1-12                             [128, 256, 4, 4]          --
├─Flatten: 1-13                          [128, 4096]               --
├─Linear: 1-14                           [128, 512]                2,097,664
├─ReLU: 1-15                             [128, 512]                --
├─Linear: 1-16                           [128, 256]                131,328
├─ReLU: 1-17                             [128, 256]                --
├─Linear: 1-18                           [128, 128]                32,896
├─Linear: 1-19                           [128, 128]                32,896
├─Linear: 1-20                           [128, 256]                33,024
├─ReLU: 1-21                             [128, 256]                --
├─Linear: 1-22                           [128, 512]                131,584
├─ReLU: 1-23                             [128, 512]                --
├─Linear: 1-24                           [128, 4096]               2,101,248
├─ReLU: 1-25                             [128, 4096]               --
├─ConvTranspose2d: 1-26                  [128, 128, 8, 8]          295,040
├─BatchNorm2d: 1-27                      [128, 128, 8, 8]          256
├─ReLU: 1-28                             [128, 128, 8, 8]          --
├─ConvTranspose2d: 1-29                  [128, 64, 16, 16]         73,792
├─BatchNorm2d: 1-30                      [128, 64, 16, 16]         128
├─ReLU: 1-31                             [128, 64, 16, 16]         --
├─ConvTranspose2d: 1-32                  [128, 32, 32, 32]         18,464
├─BatchNorm2d: 1-33                      [128, 32, 32, 32]         64
├─ReLU: 1-34                             [128, 32, 32, 32]         --
├─ConvTranspose2d: 1-35                  [128, 3, 64, 64]          867
├─BatchNorm2d: 1-36                      [128, 3, 64, 64]          6
├─Sigmoid: 1-37                          [128, 3, 64, 64]          --
==========================================================================================
Total params: 5,338,633
Trainable params: 5,338,633
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 10.23
==========================================================================================
Input size (MB): 6.29
Forward/backward pass size (MB): 274.46
Params size (MB): 21.35
Estimated Total Size (MB): 302.11
==========================================================================================
Epoch: 0 [Batch: 100]	Loss: 0.5867632031440735
Epoch: 0 [Batch: 200]	Loss: 0.5732694864273071
Epoch: 0 [Batch: 300]	Loss: 0.5558316707611084
Epoch: 0 [Batch: 400]	Loss: 0.5450615882873535
Epoch: 0 [Batch: 500]	Loss: 0.5427708625793457
Epoch: 0 [Batch: 600]	Loss: 0.5279189348220825
Epoch: 0 [Batch: 700]	Loss: 0.5247572064399719
Epoch: 0 [Batch: 800]	Loss: 0.528165340423584
Epoch: 0 [Batch: 900]	Loss: 0.5119712352752686
Epoch: 0 [Batch: 1000]	Loss: 0.515582799911499
Epoch: 0 [Batch: 1100]	Loss: 0.5294598937034607
Epoch: 0 [Batch: 1200]	Loss: 0.5306448340415955
Saving model checkpoint at epoch: 0
Epoch: 0 :::::::::::::: Train loss: 0.5420158013736611
Epoch: 0 :::::::::::::: Test loss: 0.5367423361347567
Epoch: 1 [Batch: 100]	Loss: 0.5222868919372559
Epoch: 1 [Batch: 200]	Loss: 0.5083505511283875
Epoch: 1 [Batch: 300]	Loss: 0.5209038853645325
Epoch: 1 [Batch: 400]	Loss: 0.5132976174354553
Epoch: 1 [Batch: 500]	Loss: 0.5067513585090637
Epoch: 1 [Batch: 600]	Loss: 0.5111842751502991
Epoch: 1 [Batch: 700]	Loss: 0.5048030614852905
Epoch: 1 [Batch: 800]	Loss: 0.5126513242721558
Epoch: 1 [Batch: 900]	Loss: 0.5206387042999268
Epoch: 1 [Batch: 1000]	Loss: 0.49998509883880615
Epoch: 1 [Batch: 1100]	Loss: 0.5069875121116638
Epoch: 1 [Batch: 1200]	Loss: 0.5135248303413391
Epoch: 1 :::::::::::::: Train loss: 0.5140810371570752
Epoch: 1 :::::::::::::: Test loss: 0.5307863212400867
Epoch: 2 [Batch: 100]	Loss: 0.5251048803329468
Epoch: 2 [Batch: 200]	Loss: 0.5150017142295837
Epoch: 2 [Batch: 300]	Loss: 0.5033590793609619
Epoch: 2 [Batch: 400]	Loss: 0.5141885280609131
Epoch: 2 [Batch: 500]	Loss: 0.5052472352981567
Epoch: 2 [Batch: 600]	Loss: 0.5137069821357727
Epoch: 2 [Batch: 700]	Loss: 0.5091282725334167
Epoch: 2 [Batch: 800]	Loss: 0.5090819597244263
Epoch: 2 [Batch: 900]	Loss: 0.5044626593589783
Epoch: 2 [Batch: 1000]	Loss: 0.48654234409332275
Epoch: 2 [Batch: 1100]	Loss: 0.5003454089164734
Epoch: 2 [Batch: 1200]	Loss: 0.5149620175361633
Saving model checkpoint at epoch: 2
Epoch: 2 :::::::::::::: Train loss: 0.5098192600356041
Epoch: 2 :::::::::::::: Test loss: 0.5269179094222284
Epoch: 3 [Batch: 100]	Loss: 0.5011255741119385
Epoch: 3 [Batch: 200]	Loss: 0.513586699962616
Epoch: 3 [Batch: 300]	Loss: 0.5099585652351379
Epoch: 3 [Batch: 400]	Loss: 0.49370211362838745
Epoch: 3 [Batch: 500]	Loss: 0.5121460556983948
Epoch: 3 [Batch: 600]	Loss: 0.5141807198524475
Epoch: 3 [Batch: 700]	Loss: 0.5120932459831238
Epoch: 3 [Batch: 800]	Loss: 0.503804087638855
Epoch: 3 [Batch: 900]	Loss: 0.5008921027183533
Epoch: 3 [Batch: 1000]	Loss: 0.5159016251564026
Epoch: 3 [Batch: 1100]	Loss: 0.49966374039649963
Epoch: 3 [Batch: 1200]	Loss: 0.501560389995575
Epoch: 3 :::::::::::::: Train loss: 0.5076591165679163
Epoch: 3 :::::::::::::: Test loss: 0.5274386528999575
Epoch: 4 [Batch: 100]	Loss: 0.5007526874542236
Epoch: 4 [Batch: 200]	Loss: 0.5175467133522034
Epoch: 4 [Batch: 300]	Loss: 0.4970256984233856
Epoch: 4 [Batch: 400]	Loss: 0.502056360244751
Epoch: 4 [Batch: 500]	Loss: 0.5163634419441223
Epoch: 4 [Batch: 600]	Loss: 0.49797332286834717
Epoch: 4 [Batch: 700]	Loss: 0.5127643346786499
Epoch: 4 [Batch: 800]	Loss: 0.5110644698143005
Epoch: 4 [Batch: 900]	Loss: 0.5032073259353638
Epoch: 4 [Batch: 1000]	Loss: 0.5008459687232971
Epoch: 4 [Batch: 1100]	Loss: 0.5091355443000793
Epoch: 4 [Batch: 1200]	Loss: 0.5015829801559448
Saving model checkpoint at epoch: 4
Epoch: 4 :::::::::::::: Train loss: 0.506256450568476
Epoch: 4 :::::::::::::: Test loss: 0.5245845498577241
Epoch: 5 [Batch: 100]	Loss: 0.5050608515739441
Epoch: 5 [Batch: 200]	Loss: 0.5047774910926819
Epoch: 5 [Batch: 300]	Loss: 0.5118760466575623
Epoch: 5 [Batch: 400]	Loss: 0.499161034822464
Epoch: 5 [Batch: 500]	Loss: 0.506395161151886
Epoch: 5 [Batch: 600]	Loss: 0.5073932409286499
Epoch: 5 [Batch: 700]	Loss: 0.5070634484291077
Epoch: 5 [Batch: 800]	Loss: 0.5160980820655823
Epoch: 5 [Batch: 900]	Loss: 0.508111834526062
Epoch: 5 [Batch: 1000]	Loss: 0.503677248954773
Epoch: 5 [Batch: 1100]	Loss: 0.5092636346817017
Epoch: 5 [Batch: 1200]	Loss: 0.5056572556495667
Epoch: 5 :::::::::::::: Train loss: 0.5052501382225616
Epoch: 5 :::::::::::::: Test loss: 0.5229790833688551
Epoch: 6 [Batch: 100]	Loss: 0.5093674063682556
Epoch: 6 [Batch: 200]	Loss: 0.4978391230106354
Epoch: 6 [Batch: 300]	Loss: 0.49822452664375305
Epoch: 6 [Batch: 400]	Loss: 0.4988304674625397
Epoch: 6 [Batch: 500]	Loss: 0.5039244294166565
Epoch: 6 [Batch: 600]	Loss: 0.5004119873046875
Epoch: 6 [Batch: 700]	Loss: 0.5051856637001038
Epoch: 6 [Batch: 800]	Loss: 0.5067123770713806
Epoch: 6 [Batch: 900]	Loss: 0.5042108297348022
Epoch: 6 [Batch: 1000]	Loss: 0.49760153889656067
Epoch: 6 [Batch: 1100]	Loss: 0.5132306218147278
Epoch: 6 [Batch: 1200]	Loss: 0.5005210041999817
Saving model checkpoint at epoch: 6
Epoch: 6 :::::::::::::: Train loss: 0.5044107017650274
Epoch: 6 :::::::::::::: Test loss: 0.5224661323332017
Epoch: 7 [Batch: 100]	Loss: 0.4971883296966553
Epoch: 7 [Batch: 200]	Loss: 0.5017702579498291
Epoch: 7 [Batch: 300]	Loss: 0.5136513113975525
Epoch: 7 [Batch: 400]	Loss: 0.5023003816604614
Epoch: 7 [Batch: 500]	Loss: 0.5094058513641357
Epoch: 7 [Batch: 600]	Loss: 0.5017158389091492
Epoch: 7 [Batch: 700]	Loss: 0.5036344528198242
Epoch: 7 [Batch: 800]	Loss: 0.5036140084266663
Epoch: 7 [Batch: 900]	Loss: 0.48964086174964905
Epoch: 7 [Batch: 1000]	Loss: 0.5149381160736084
Epoch: 7 [Batch: 1100]	Loss: 0.5097863674163818
Epoch: 7 [Batch: 1200]	Loss: 0.4991248846054077
Epoch: 7 :::::::::::::: Train loss: 0.5037982244129541
Epoch: 7 :::::::::::::: Test loss: 0.5216167073095999
Epoch: 8 [Batch: 100]	Loss: 0.4974684417247772
Epoch: 8 [Batch: 200]	Loss: 0.49779799580574036
Epoch: 8 [Batch: 300]	Loss: 0.501987874507904
Epoch: 8 [Batch: 400]	Loss: 0.4945172965526581
Epoch: 8 [Batch: 500]	Loss: 0.5063005685806274
Epoch: 8 [Batch: 600]	Loss: 0.5084637999534607
Epoch: 8 [Batch: 700]	Loss: 0.5161787867546082
Epoch: 8 [Batch: 800]	Loss: 0.5156539678573608
Epoch: 8 [Batch: 900]	Loss: 0.5129126310348511
Epoch: 8 [Batch: 1000]	Loss: 0.5088329911231995
Epoch: 8 [Batch: 1100]	Loss: 0.5000379085540771
Epoch: 8 [Batch: 1200]	Loss: 0.5022441148757935
Saving model checkpoint at epoch: 8
Epoch: 8 :::::::::::::: Train loss: 0.503272205544306
Epoch: 8 :::::::::::::: Test loss: 0.5201540796987472
Epoch: 9 [Batch: 100]	Loss: 0.5002370476722717
Epoch: 9 [Batch: 200]	Loss: 0.4891463816165924
Epoch: 9 [Batch: 300]	Loss: 0.4947793781757355
Epoch: 9 [Batch: 400]	Loss: 0.5063551664352417
Epoch: 9 [Batch: 500]	Loss: 0.5035309195518494
Epoch: 9 [Batch: 600]	Loss: 0.5007392168045044
Epoch: 9 [Batch: 700]	Loss: 0.5023465752601624
Epoch: 9 [Batch: 800]	Loss: 0.5041490197181702
Epoch: 9 [Batch: 900]	Loss: 0.5150712728500366
Epoch: 9 [Batch: 1000]	Loss: 0.4960901141166687
Epoch: 9 [Batch: 1100]	Loss: 0.5206218957901001
Epoch: 9 [Batch: 1200]	Loss: 0.5034814476966858
Epoch: 9 :::::::::::::: Train loss: 0.502858277008723
Epoch: 9 :::::::::::::: Test loss: 0.5199239742371344
Epoch: 10 [Batch: 100]	Loss: 0.49684080481529236
Epoch: 10 [Batch: 200]	Loss: 0.5024967193603516
Epoch: 10 [Batch: 300]	Loss: 0.47980380058288574
Epoch: 10 [Batch: 400]	Loss: 0.4984756410121918
Epoch: 10 [Batch: 500]	Loss: 0.5033121705055237
Epoch: 10 [Batch: 600]	Loss: 0.5037786364555359
Epoch: 10 [Batch: 700]	Loss: 0.4982960820198059
Epoch: 10 [Batch: 800]	Loss: 0.4935782849788666
Epoch: 10 [Batch: 900]	Loss: 0.5010820031166077
Epoch: 10 [Batch: 1000]	Loss: 0.5075439810752869
Epoch: 10 [Batch: 1100]	Loss: 0.4944612383842468
Epoch: 10 [Batch: 1200]	Loss: 0.48976561427116394
Saving model checkpoint at epoch: 10
Epoch: 10 :::::::::::::: Train loss: 0.5025664896921883
Epoch: 10 :::::::::::::: Test loss: 0.5195576836985927
Epoch: 11 [Batch: 100]	Loss: 0.5089313983917236
Epoch: 11 [Batch: 200]	Loss: 0.5096104741096497
Epoch: 11 [Batch: 300]	Loss: 0.4993554651737213
Epoch: 11 [Batch: 400]	Loss: 0.4947773516178131
Epoch: 11 [Batch: 500]	Loss: 0.5011646151542664
Epoch: 11 [Batch: 600]	Loss: 0.5099595785140991
Epoch: 11 [Batch: 700]	Loss: 0.5003136992454529
Epoch: 11 [Batch: 800]	Loss: 0.5127391219139099
Epoch: 11 [Batch: 900]	Loss: 0.49714604020118713
Epoch: 11 [Batch: 1000]	Loss: 0.49681171774864197
Epoch: 11 [Batch: 1100]	Loss: 0.5039438009262085
Epoch: 11 [Batch: 1200]	Loss: 0.48773205280303955
Epoch: 11 :::::::::::::: Train loss: 0.5021657890733258
Epoch: 11 :::::::::::::: Test loss: 0.518528401851654
Epoch: 12 [Batch: 100]	Loss: 0.4936767518520355
Epoch: 12 [Batch: 200]	Loss: 0.5017299652099609
Epoch: 12 [Batch: 300]	Loss: 0.4944687485694885
Epoch: 12 [Batch: 400]	Loss: 0.5046254992485046
Epoch: 12 [Batch: 500]	Loss: 0.5050814151763916
Epoch: 12 [Batch: 600]	Loss: 0.5006794333457947
Epoch: 12 [Batch: 700]	Loss: 0.5004799365997314
Epoch: 12 [Batch: 800]	Loss: 0.4920632839202881
Epoch: 12 [Batch: 900]	Loss: 0.5037407279014587
Epoch: 12 [Batch: 1000]	Loss: 0.5000444054603577
Epoch: 12 [Batch: 1100]	Loss: 0.4996044337749481
Epoch: 12 [Batch: 1200]	Loss: 0.4953455626964569
Saving model checkpoint at epoch: 12
Epoch: 12 :::::::::::::: Train loss: 0.5016938030110111
Epoch: 12 :::::::::::::: Test loss: 0.5187626169573876
Epoch: 13 [Batch: 100]	Loss: 0.5045956373214722
Epoch: 13 [Batch: 200]	Loss: 0.5007182359695435
Epoch: 13 [Batch: 300]	Loss: 0.49246692657470703
Epoch: 13 [Batch: 400]	Loss: 0.5132979154586792
Epoch: 13 [Batch: 500]	Loss: 0.4910406768321991
Epoch: 13 [Batch: 600]	Loss: 0.5106946229934692
Epoch: 13 [Batch: 700]	Loss: 0.5062838196754456
Epoch: 13 [Batch: 800]	Loss: 0.5000196695327759
Epoch: 13 [Batch: 900]	Loss: 0.5064682960510254
Epoch: 13 [Batch: 1000]	Loss: 0.4941294491291046
Epoch: 13 [Batch: 1100]	Loss: 0.5042250156402588
Epoch: 13 [Batch: 1200]	Loss: 0.48930540680885315
Epoch: 13 :::::::::::::: Train loss: 0.5013070835737052
Epoch: 13 :::::::::::::: Test loss: 0.517837756295358
Epoch: 14 [Batch: 100]	Loss: 0.5034117102622986
Epoch: 14 [Batch: 200]	Loss: 0.5041162967681885
Epoch: 14 [Batch: 300]	Loss: 0.495199590921402
Epoch: 14 [Batch: 400]	Loss: 0.5097292065620422
Epoch: 14 [Batch: 500]	Loss: 0.4887566864490509
Epoch: 14 [Batch: 600]	Loss: 0.49858254194259644
Epoch: 14 [Batch: 700]	Loss: 0.5051552057266235
Epoch: 14 [Batch: 800]	Loss: 0.5066375732421875
Epoch: 14 [Batch: 900]	Loss: 0.49605390429496765
Epoch: 14 [Batch: 1000]	Loss: 0.49749666452407837
Epoch: 14 [Batch: 1100]	Loss: 0.5106223225593567
Epoch: 14 [Batch: 1200]	Loss: 0.48879358172416687
Saving model checkpoint at epoch: 14
Epoch: 14 :::::::::::::: Train loss: 0.5011003532107658
Epoch: 14 :::::::::::::: Test loss: 0.5177153523891203
Epoch: 15 [Batch: 100]	Loss: 0.5030969381332397
Epoch: 15 [Batch: 200]	Loss: 0.4960065186023712
Epoch: 15 [Batch: 300]	Loss: 0.4990137815475464
Epoch: 15 [Batch: 400]	Loss: 0.49408406019210815
Epoch: 15 [Batch: 500]	Loss: 0.4933722913265228
Epoch: 15 [Batch: 600]	Loss: 0.5059793591499329
Epoch: 15 [Batch: 700]	Loss: 0.49198535084724426
Epoch: 15 [Batch: 800]	Loss: 0.4982117712497711
Epoch: 15 [Batch: 900]	Loss: 0.4998137056827545
Epoch: 15 [Batch: 1000]	Loss: 0.4997517764568329
Epoch: 15 [Batch: 1100]	Loss: 0.5094031095504761
Epoch: 15 [Batch: 1200]	Loss: 0.5088320970535278
Epoch: 15 :::::::::::::: Train loss: 0.5008551444386423
Epoch: 15 :::::::::::::: Test loss: 0.5172867880713555
Epoch: 16 [Batch: 100]	Loss: 0.48845750093460083
Epoch: 16 [Batch: 200]	Loss: 0.49682125449180603
Epoch: 16 [Batch: 300]	Loss: 0.49582475423812866
Epoch: 16 [Batch: 400]	Loss: 0.5172271132469177
Epoch: 16 [Batch: 500]	Loss: 0.4995899200439453
Epoch: 16 [Batch: 600]	Loss: 0.5030831694602966
Epoch: 16 [Batch: 700]	Loss: 0.49764737486839294
Epoch: 16 [Batch: 800]	Loss: 0.4898243844509125
Epoch: 16 [Batch: 900]	Loss: 0.5029110312461853
Epoch: 16 [Batch: 1000]	Loss: 0.5020649433135986
Epoch: 16 [Batch: 1100]	Loss: 0.5190306305885315
Epoch: 16 [Batch: 1200]	Loss: 0.5076005458831787
Saving model checkpoint at epoch: 16
Epoch: 16 :::::::::::::: Train loss: 0.5006734106739519
Epoch: 16 :::::::::::::: Test loss: 0.5170186969541735
Epoch: 17 [Batch: 100]	Loss: 0.5053790211677551
Epoch: 17 [Batch: 200]	Loss: 0.5028060078620911
Epoch: 17 [Batch: 300]	Loss: 0.5047109723091125
Epoch: 17 [Batch: 400]	Loss: 0.4995616674423218
Epoch: 17 [Batch: 500]	Loss: 0.49130743741989136
Epoch: 17 [Batch: 600]	Loss: 0.5046616792678833
Epoch: 17 [Batch: 700]	Loss: 0.5077399611473083
Epoch: 17 [Batch: 800]	Loss: 0.5055715441703796
Epoch: 17 [Batch: 900]	Loss: 0.5011171102523804
Epoch: 17 [Batch: 1000]	Loss: 0.5008561611175537
Epoch: 17 [Batch: 1100]	Loss: 0.4988679885864258
Epoch: 17 [Batch: 1200]	Loss: 0.4983230531215668
Epoch: 17 :::::::::::::: Train loss: 0.5005017108892851
Epoch: 17 :::::::::::::: Test loss: 0.516089722418016
Epoch: 18 [Batch: 100]	Loss: 0.49654412269592285
Epoch: 18 [Batch: 200]	Loss: 0.4996093809604645
Epoch: 18 [Batch: 300]	Loss: 0.49290066957473755
Epoch: 18 [Batch: 400]	Loss: 0.4909449517726898
Epoch: 18 [Batch: 500]	Loss: 0.5158463716506958
Epoch: 18 [Batch: 600]	Loss: 0.49872902035713196
Epoch: 18 [Batch: 700]	Loss: 0.49372899532318115
Epoch: 18 [Batch: 800]	Loss: 0.5084972381591797
Epoch: 18 [Batch: 900]	Loss: 0.4898979961872101
Epoch: 18 [Batch: 1000]	Loss: 0.486835777759552
Epoch: 18 [Batch: 1100]	Loss: 0.49890685081481934
Epoch: 18 [Batch: 1200]	Loss: 0.5042402148246765
Saving model checkpoint at epoch: 18
Epoch: 18 :::::::::::::: Train loss: 0.5003433021197256
Epoch: 18 :::::::::::::: Test loss: 0.5165334090109794
Epoch: 19 [Batch: 100]	Loss: 0.5053050518035889
Epoch: 19 [Batch: 200]	Loss: 0.5051671266555786
Epoch: 19 [Batch: 300]	Loss: 0.501289963722229
Epoch: 19 [Batch: 400]	Loss: 0.506455659866333
Epoch: 19 [Batch: 500]	Loss: 0.5023331046104431
Epoch: 19 [Batch: 600]	Loss: 0.510572075843811
Epoch: 19 [Batch: 700]	Loss: 0.490482360124588
Epoch: 19 [Batch: 800]	Loss: 0.5143277645111084
Epoch: 19 [Batch: 900]	Loss: 0.5140016078948975
Epoch: 19 [Batch: 1000]	Loss: 0.4901103079319
Epoch: 19 [Batch: 1100]	Loss: 0.4942249655723572
Epoch: 19 [Batch: 1200]	Loss: 0.4929971992969513
Epoch: 19 :::::::::::::: Train loss: 0.5002021607125865
Epoch: 19 :::::::::::::: Test loss: 0.5160162821892769
Training time: 26.62086420059204 minutes
